\chapter{The Integral}
\section{Mean Value Theorem}
\subsection{The Mean Value Theorem (MVT)}
If $x(t)$ is continuous on $a\le t\le b$, and differentiable on $a<t<b$, that is, $x^\prime(t)$ is defined for all $t$, $a<t<b$, then
\[
\frac{x(b)-x(a)}{b-a}=x^\prime(c)\text{ for some $c$, with $a<c<b$.}
\]
Equivalently, in geometric terms, there is at least one point $c$, with $a<c<b$, at which the tangent line is parallel to the secant line through $(a,x(a))$ and $(b,x(b))$:
\todo{Supplement figure}
\subsection{Upper and Lower Bounds}
A number $M$ is an \textbf{upper bound} of a function $f(x)$ if
\[f(x)\le M\text{ for all }x\]
and a number $m$ is a \textbf{lower bound} of a function $f(x)$ if
\[m\le f(x)\text{ for all }x\]

We can consider upper and lower bounds on the entire real number line, or on an interval.
\[m\le f(x)\le M\]
\todo{Supplement figure}
In other words, an upper bound of a function is a number that is larger than or equal to all value of the function. A lower bound of a function is a number which is smaller than or equal to all values of the function.
\subsection{Old news}
We have been relying on the following fundamental facts whenever we try to understand a function using its derivative. But in face, these faces are consequences of the mean value theorem.
\begin{itemize}
\item If $x^\prime(t)\ge0$ for all $t$ in $(A,B)$, then $x(t)$ is \textbf{increasing or staying the same} over $[A,B]$.
\item IF $x^\prime(t)>0$ for all $t$ in $(A,B)$, then $x(t)$ is \textbf{strictly increasing} over $[A,B]$.
\item IF $x^\prime(t)\le0$ for all $t$ in $(A,B)$, then $x(t)$ is \textbf{decreasing or staying the same} over $[A,B]$.
\item IF $x^\prime(t)<0$ for all $t$ in $(A,B)$, then $x(t)$ is \textbf{strictly decreasing} over $[A,B]$.
\item IF $x^\prime(t)=0$ for all $t$ in $(A,B)$, then $x(t)$ is \textbf{constant} over $[A,B]$.
\end{itemize}
\subsection{Bounding the average rate of change}
The equality in the MVT can be used to restrict the range of possible values of the average rate of change and the total change. More precisely, if there are numbers $m$ and $M$ such that
\[m\le x^\prime(c)\le M\text{ for all $c$ with }a<c<b,\]
that is, $m$ is a lower bound and $M$ is an upper bound on $x^\prime(c)$ over $(a,b)$, then the MVT implies the following
\begin{gather*}
m\le\frac{x(b)-x(a)}{b-a}\le M\text{ (Bounds on the average rate of change)}\\
m(b-a)\le x(b)-x(a)\le M(b-a)\text{ (Bounds on the total change)}
\end{gather*}
In other words, the average rate of change must be in between the maximum and the minimum of the derivative, and the total change must be in between the maximum and minimum of the derivative multiplied by the length of the interval.
\section{Differentials and Antiderivatives}
\subsection{Differential notation}
Let $y=F(x)$, the \textbf{differential of $\bm{y}$} is defined as
\[\dd y=F^\prime\dd x.\]
This is also called the \textbf{differential of $\bm{F}$} and denoted $\bm{\dd F}$.

\begin{note}
Rearranging this question, we get the Leibniz notation for the derivative, which says the derivative is the ratio of the two differentials $\dd y$ and $\dd x$.
\[F^\prime=\frac{\dd y}{\dd x}\text{ or }\frac{\dd F}{\dd x}.\]
\end{note}

We may think of the differential of $x$, $\dd x$ as a ``little bit'' of $x$, and the differential of $y$, $\dd y$, as a ``little bit'' of $y$. Here, what we mean by a ``little bit'' is really an infinitely small bit, we call these infinitely small quantities ``infinitesimal.'' The point is that even though both $\dd y$ and $\dd x$ are infinitely small, their ratio is \textbf{not}. Their ratio is the derivative $F^\prime(x)$. In other words, the differential notation says that $\dd y$ is proportional to $\dd x$ with constant of proportionality $F^\prime(x)$ even though both $\dd y$ and $\dd x$ are infinitely small. We use the differential notation as a tool to keep track of how much $y$ changes when $x$ changes by a tiny tiny\ldots tiny bit.

The geometric picture for the differential is the same as that for linear approximation.
\todo{Supplement figure}
Let us compare the differential notation with the formula for linear approximations.
\begin{alignat*}{3}
& \text{Linear approximation at $x$:} & \: & \Delta F\approx F^\prime\Delta x\text{, $\Delta x$ is a finite change in $x$;}\\
& \text{Differential notation:}       &    & \dd F=F^\prime\dd x\text{, $\Delta x$ is a tiny tiny\ldots tiny bit of $x$.}
\end{alignat*}

\subsection{Antiderivatives}
\textbf{An antiderivative} of $f$ is any function $f$ such that
\[F^\prime=f.\]

\subsection{Indefinite integral}
Given a function $f$, the \textbf{indefinite integral} or \textbf{the antiderivative} of $f$ is denoted $\int f\dd x$. It is the family of functions
\[\int f\dd x=F(x)+C\]
where $F$ is any antiderivative of $f$, and $C$ is any constant.

We call $\int$ the \textbf{integral sign}, $f$ the \textbf{integrand}, and $C$ the \textbf{constant of integration}.

\begin{note}
The constant of integration is present in this definition because the derivative of a function determines only the shape of the function, but the derivative does not change if the function is shifted up or down by the same constant everywhere.
\end{note}

\subsection{Uniqueness of the indefinite integral}
The indefinite integral
\[\int f\dd x=F(x)+C\]
is termed ``indefinite'' since it contains an undetermined constant $C$ and is not just one function but a family of infinitely many functions, parameterized by $C$.

On the other hand, the constant is the only ambiguity of the indefinite integral due to the MVT, which guarantees that any two antiderivatives of the same function can differ only by a constant.
\begin{proof}[Why MVT applies to the statement that ``$F_1-F_2=C$'']
Suppose $G=F_1-F_2$, $F_1$ and $F_2$ are differentiable on interval $(a,b)$ (which requires $F_1$, $F_2$ to be continuous, a standard assumption), then $G^\prime=F_1^\prime-F_2^\prime=f-f=0$.

Take any two points $x_1, x_2\in(a,b)$ with $x_1<x_2$. By the MVT, since $G$ is differentiable on $(x_1, x_2)$ and continuous on $[x_1, x_2]$, there exists some $c\in(x_1,x_2)$ such that:
\[G^\prime(c)=\frac{G(x_2)-G(x_1)}{x_2-x_1}.\]

But we know $G^\prime=0$ everywhere. Thus:
\[\frac{G(x_2)-G(x_1)}{x_2-x_1}=0,\]
this implies:
\[G(x_2)-G(x_1)=0\iff G(x_2)=G(x_1).\]

Since $x_1$ and $x_2$ are arbitrary points in the interval, $G=F_1-F_2$ must be constant throughout the interval. Thus:
\[F_1-F_2=C,\]
where $C$ is a constant.
\end{proof}
\subsection{Integrals of powers}
\[\int x^p\dd x=\begin{cases}
  \frac{x^{p+1}}{p+1}+C\text{ if p\neq-1,}\\
  \ln\abs x+C\text{ if p=-1.}
\end{cases}\]
\subsection{First rules of integration}
\begin{table}[H]\begin{tabular}{lll}
                   & Integration Rules                                     & Differentiation Rules \\
Constant multiple: & $\int kf(x)\dd x=k\int f(x)\dd x$                     & $\dd(kF)=k\dd F$      \\
Sum:               & $\int (f+g)\dd x=\int f\dd x+\int g\dd x$ & $\dd(F+G)=\dd F+\dd G$ 
\end{tabular}\end{table}
On the other hand, the following naive product and quotient rules do \textbf{not} work:
\begin{gather*}
\int fg\dd x\neq\int f\dd x\cdot\int g\dd x,\\
\int \frac f g\dd x\neq\frac{\int f\dd x}{\int g\dd x}.
\end{gather*}
\subsection{Method of substitution}
The method of substitution is the integration analogue of the chain rule.

If
\[g\dd x=f(u)u^\prime\dd x,\]
that is, the differential $g\dd x$ is the result of a chain rule, then
\begin{alignat*}{1}
\int g\dd x & =\int f(u)u^\prime\dd x\\
            & =\int f(u)\dd u\\
			& =F(u)+C,
\end{alignat*}
where $F(u)$ is an antiderivative of $f(u)$.

\section{Differential equation}
\subsection{Separation of variables}
A differential equation is \textbf{separable} is it can be written in the form $\frac{\dd y}{\dd x}=f(x)g(y)$.

To solve a separable differential equation, separate all terms involving $x$ from terms involving $y$, and anti-differentiate both sides.
\begin{gather*}
\frac{\dd y}{\dd x}=f(x)g(y)\\
\frac{\dd y}{g(y)}=f(x)\dd x\\
\int\frac{\dd y}{g(y)}=\int f(x)\dd x\\
\end{gather*}
\subsection{Initial conditions}
\subsubsection{Theorem}
Given a differential equation $\frac{\dd y}{\dd x}=f(x)g(y)$ and an \textbf{initial condition} $y(a)=b$, if $f$, $g$, and $g^\prime$ are continuous near $(a,b)$, then there is a unique function $y$ whose derivative is given by $f(x)g(y)$ and that passes through the point $(a,b)$.
\subsubsection{Slope fields}
The \textbf{slope field} is a diagram that helps us to visualize the information in a first order differential equation. The slope field is obtained as follows. At each point $(x,y)$. The solution curves must be tangent to the slope field at all points.
\subsubsection{Euler's method}
Given the differential equation $\frac{\dd y}{\dd x}=x+y$:
\begin{enumerate}
\item Choose a step size $h$ (The smaller the step size, the more accurate the approximation.)
\item Let $(x_0,y_0)$ be the initial condition.
\item We can use the differential equation and step size to determine the value of the function at $x_1$:
  \begin{align*}
  & x_1=x_0+h\\
  & y_1=y_0+(x_0+y_0)h\quad\text{(linear approximation)}
  \end{align*}
\item Iterate this process:
  \begin{align*}
  & x_{k+1}=x_k+h\\
  & y_{k+1}=y_k+(x_k+y_k)h\quad\text{(linear approximation)}
  \end{align*}
\end{enumerate}
\section{Modeling a zipline}
\subsection{Hyperbolic Sine and Cosine}
Recall that the hyperbolic coisine and sine are defined by the relationships
\begin{gather*}
\cosh t=\frac{e^t+e^{-t}}2\\
\sinh t=\frac{e^t-e^{-t}}2
\end{gather*}
The basic trigonometric functions are related to the geometry of a circle. These hyperbolic trig functions are related to the geometry of a hyperbola.
\subsection{Geometric description of trig functions}
The point labeled in the image, $(\cos\theta,\sin\theta)$, is defined to be the point on the circle $x^2+y^2=1$ such that the shaded area is $\theta$.
\todo{Supplement figure}

A circle of radius 1 centered at the origin is plotted in the $xy$-plane. A point is indicated on the circle in the first quadrant and makes an angle of theta with the positive $x$-axis. A portion of the circle is shaded that indicates the area swept over the range of angles from $-\theta$ to $\theta$.
\subsection{Geometric description of hyperbolic trig functions}
The point labeled in the image, $(\cosh t, \sinh t)$, is defined to be the point on the hyperbola $x^2-y^2=1$ such that the area of the shaded region is $t$.
\todo{Supplement figure}
\todo{Supplement modeling of the zipline in part 4}
\section{The definite integral}
\subsection{Geometric definition of the definite integral}
The \textbf{definite integral of $f$ from $a$ to $b$}, denoted by
\[\int_a^bf(x)\dd x,\]
is the area of the region above the $x$-axis, below the curve $y=f(x)$, and in between the two vertical lines $x=a$ and $x=b$, as shown shaded in the figure below.

The $x$-values $a$ and $b$ are called the \textbf{lower} and \textbf{upper limits of the integral}. (This is a different sense of the word ``limit'' from when we take the limit of a function.)
\todo{Supplement figure}

The only difference between the notation for definite and indefinite integrals is that definite integrals have limits but indefinite integrals do not.
\subsection{Summation notation}
The $\sum$ notation in a compact way to denote a sum in which each term is obtained from a formula:
\[\sum_{i=1}^na_i=a_1+a_2+\cdots+a_{n-1}+a_n\]
where $i$ indexes the terms, and $a_i$ is a formula for the $i$\textsuperscript{th} term of the sum.

The notation $\sum_{i=1}^na_i$ reads ``the sum of $a_i$ from $i=1$ to $i=n$.''

For example, if $a_i=i^2$, that is, the formula for the term indexed by $i$ is $i^2$, then
\[\sum_{i=1}^ni^2=1^2+2^2+3^2+\cdots+(n-1)^2+n^2\]
\subsection{Riemann sums}
Let us summarize in precise terms the steps for evaluating $\int_a^bf(x)\dd x$ using a Riemann Sum.
\begin{enumerate}
\item Divide $[a,b]$ into $n$ equal subintervals.
  \todo{Supplement figure}\\
  Then each interval is of length
  \[\Delta x=\frac{b-a}n.\]
  Let the $i$\textsuperscript{th} subinterval be the \textbf{base} of $i$\textsuperscript{th} rectangle.
\item Choose a point $c_i$ within the $i$\textsuperscript{th} subinterval. Choose $f(c_i)$ be the \textbf{height} of the $i$\textsuperscript{th} rectangle.\todo{Supplement figure}
\item Add up the areas of the $n$ rectangles. The total area of $n$ rectangles is:
  \[\underbrace{f(c_1)}_{\text{height}}\underbrace{\Delta x}_{\text{base}}
  +\underbrace{f(c_2)}_{\text{height}}\underbrace{\Delta x}_{\text{base}}
  +\cdots
  +\underbrace{f(c_n)}_{\text{height}}\underbrace{\Delta x}_{\text{base}}
  =\sum_{i=1}^nf(c_i)\Delta x
  \]
\item Take the limit as the rectangles become infinitesimally thin, ($\Delta x\to0$, or equivalent $n\to\infty$). This limit is the actual area under the curve between $a$ and $b$.
  \[\lim_{n\to\infty}\sum_{i=1}^nf(c_i)\Delta x=\int_a^bf(x)\dd x\]
  The sum of the areas of the $n$ rectangles, $\sum_{i=1}^nf(c_i)\Delta x$, is called a \textbf{Riemann Sum}. If we pick $c_i$ to be the left endpoint of the $i$\textsuperscript{th} subinterval, the Riemann sum is called a \textbf{left Riemann Sum}. Similarly, if $c_i$ is the right endpoint of the $i$\textsuperscript{th} interval, the Riemann sum is called a \textbf{right Riemann Sum}.

  However, in the limit $n\to\infty$ (so that $\Delta x\to0$), this distinction is no longer needed. The limit of any Riemann Sum, no matter what the $c_i$'s within the subinterval are, is equal to the exact area under the curve.
\end{enumerate}
\subsection{Cumulative sums}
We have been defining the definite integral $\int_a^bf(x)\dd x$ geometrically as the area under a curve. But $f(x)$ and $x$ can represent quantities other than length.

For instance, if $x$ represents time, with units of hours, and $f(x)$ represents velocity, with units of \unit[per-mode=single-symbol]{\km\per\hour}. So the units of the integral, $\int_a^bf(x)\dd x$, is \unit{\km}, which are the units of distance. But the integral $\int_a^bf(x)\dd x$ still represents the area under the curve, so the units of this area are \unit{\km}.

Most applications of the integral will involve seeing it as a cumulative sum.
