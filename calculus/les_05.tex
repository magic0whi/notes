\chapter{The Integral}
\section{Mean Value Theorem}
\subsection{The Mean Value Theorem (MVT)}
If $x(t)$ is continuous on $a\le t\le b$, and differentiable on $a<t<b$, that is, $x^\prime(t)$ is defined for all $t$, $a<t<b$, then
\[
\frac{x(b)-x(a)}{b-a}=x^\prime(c)\text{ for some $c$, with $a<c<b$.}
\]
Equivalently, in geometric terms, there is at least one point $c$, with $a<c<b$, at which the tangent line is parallel to the secant line through $(a,x(a))$ and $(b,x(b))$:
\todo{Supplement figure}
\subsection{Upper and Lower Bounds}
A number $M$ is an \textbf{upper bound} of a function $f(x)$ if
\[f(x)\le M\text{ for all }x\]
and a number $m$ is a \textbf{lower bound} of a function $f(x)$ if
\[m\le f(x)\text{ for all }x\]

We can consider upper and lower bounds on the entire real number line, or on an interval.
\[m\le f(x)\le M\]
\todo{Supplement figure}
In other words, an upper bound of a function is a number that is larger than or equal to all value of the function. A lower bound of a function is a number which is smaller than or equal to all values of the function.
\subsection{Old news}
We have been relying on the following fundamental facts whenever we try to understand a function using its derivative. But in face, these faces are consequences of the mean value theorem.
\begin{itemize}
\item If $x^\prime(t)\ge0$ for all $t$ in $(A,B)$, then $x(t)$ is \textbf{increasing or staying the same} over $[A,B]$.
\item IF $x^\prime(t)>0$ for all $t$ in $(A,B)$, then $x(t)$ is \textbf{strictly increasing} over $[A,B]$.
\item IF $x^\prime(t)\le0$ for all $t$ in $(A,B)$, then $x(t)$ is \textbf{decreasing or staying the same} over $[A,B]$.
\item IF $x^\prime(t)<0$ for all $t$ in $(A,B)$, then $x(t)$ is \textbf{strictly decreasing} over $[A,B]$.
\item IF $x^\prime(t)=0$ for all $t$ in $(A,B)$, then $x(t)$ is \textbf{constant} over $[A,B]$.
\end{itemize}
\subsection{Bounding the average rate of change}
The equality in the MVT can be used to restrict the range of possible values of the average rate of change and the total change. More precisely, if there are numbers $m$ and $M$ such that
\[m\le x^\prime(c)\le M\text{ for all $c$ with }a<c<b,\]
that is, $m$ is a lower bound and $M$ is an upper bound on $x^\prime(c)$ over $(a,b)$, then the MVT implies the following
\begin{gather*}
m\le\frac{x(b)-x(a)}{b-a}\le M\text{ (Bounds on the average rate of change)}\\
m(b-a)\le x(b)-x(a)\le M(b-a)\text{ (Bounds on the total change)}
\end{gather*}
In other words, the average rate of change must be in between the maximum and the minimum of the derivative, and the total change must be in between the maximum and minimum of the derivative multiplied by the length of the interval.
\section{Differentials and Antiderivatives}
\subsection{Differential notation}
Let $y=F(x)$, the \textbf{differential of $\bm{y}$} is defined as
\[\dd y=F^\prime\dd x.\]
This is also called the \textbf{differential of $\bm{F}$} and denoted $\bm{\dd F}$.

\begin{note}
Rearranging this question, we get the Leibniz notation for the derivative, which says the derivative is the ratio of the two differentials $\dd y$ and $\dd x$.
\[F^\prime=\frac{\dd y}{\dd x}\text{ or }\frac{\dd F}{\dd x}.\]
\end{note}

We may think of the differential of $x$, $\dd x$ as a ``little bit'' of $x$, and the differential of $y$, $\dd y$, as a ``little bit'' of $y$. Here, what we mean by a ``little bit'' is really an infinitely small bit, we call these infinitely small quantities ``infinitesimal.'' The point is that even though both $\dd y$ and $\dd x$ are infinitely small, their ratio is \textbf{not}. Their ratio is the derivative $F^\prime(x)$. In other words, the differential notation says that $\dd y$ is proportional to $\dd x$ with constant of proportionality $F^\prime(x)$ even though both $\dd y$ and $\dd x$ are infinitely small. We use the differential notation as a tool to keep track of how much $y$ changes when $x$ changes by a tiny tiny\ldots tiny bit.

The geometric picture for the differential is the same as that for linear approximation.
\todo{Supplement figure}
Let us compare the differential notation with the formula for linear approximations.
\begin{alignat*}{3}
& \text{Linear approximation at $x$:} & \: & \Delta F\approx F^\prime\Delta x\text{, $\Delta x$ is a finite change in $x$;}\\
& \text{Differential notation:}       &    & \dd F=F^\prime\dd x\text{, $\Delta x$ is a tiny tiny\ldots tiny bit of $x$.}
\end{alignat*}

\subsection{Antiderivatives}
\textbf{An antiderivative} of $f$ is any function $f$ such that
\[F^\prime=f.\]

\subsection{Indefinite integral}
Given a function $f$, the \textbf{indefinite integral} or \textbf{the antiderivative} of $f$ is denoted $\int f\dd x$. It is the family of functions
\[\int f\dd x=F(x)+C\]
where $F$ is any antiderivative of $f$, and $C$ is any constant.

We call $\int$ the \textbf{integral sign}, $f$ the \textbf{integrand}, and $C$ the \textbf{constant of integration}.

\begin{note}
The constant of integration is present in this definition because the derivative of a function determines only the shape of the function, but the derivative does not change if the function is shifted up or down by the same constant everywhere.
\end{note}

\subsection{Uniqueness of the indefinite integral}
The indefinite integral
\[\int f\dd x=F(x)+C\]
is termed ``indefinite'' since it contains an undetermined constant $C$ and is not just one function but a family of infinitely many functions, parameterized by $C$.

On the other hand, the constant is the only ambiguity of the indefinite integral due to the MVT, which guarantees that any two antiderivatives of the same function can differ only by a constant.
\begin{proof}[Why MVT applies to the statement that ``$F_1-F_2=C$'']
Suppose $G=F_1-F_2$, $F_1$ and $F_2$ are differentiable on interval $(a,b)$ (which requires $F_1$, $F_2$ to be continuous, a standard assumption), then $G^\prime=F_1^\prime-F_2^\prime=f-f=0$.

Take any two points $x_1, x_2\in(a,b)$ with $x_1<x_2$. By the MVT, since $G$ is differentiable on $(x_1, x_2)$ and continuous on $[x_1, x_2]$, there exists some $c\in(x_1,x_2)$ such that:
\[G^\prime(c)=\frac{G(x_2)-G(x_1)}{x_2-x_1}.\]

But we know $G^\prime=0$ everywhere. Thus:
\[\frac{G(x_2)-G(x_1)}{x_2-x_1}=0,\]
this implies:
\[G(x_2)-G(x_1)=0\iff G(x_2)=G(x_1).\]

Since $x_1$ and $x_2$ are arbitrary points in the interval, $G=F_1-F_2$ must be constant throughout the interval. Thus:
\[F_1-F_2=C,\]
where $C$ is a constant.
\end{proof}
\subsection{Integrals of powers}
\[\int x^p\dd x=\begin{cases}
  \frac{x^{p+1}}{p+1}+C\text{ if p\neq-1,}\\
  \ln\abs x+C\text{ if p=-1.}
\end{cases}\]
\subsection{First rules of integration}
\begin{table}[H]\begin{tabular}{lll}
                   & Integration Rules                                     & Differentiation Rules \\
Constant multiple: & $\int kf(x)\dd x=k\int f(x)\dd x$                     & $\dd(kF)=k\dd F$      \\
Sum:               & $\int (f+g)\dd x=\int f\dd x+\int g\dd x$ & $\dd(F+G)=\dd F+\dd G$ 
\end{tabular}\end{table}
On the other hand, the following naive product and quotient rules do \textbf{not} work:
\begin{gather*}
\int fg\dd x\neq\int f\dd x\cdot\int g\dd x,\\
\int \frac f g\dd x\neq\frac{\int f\dd x}{\int g\dd x}.
\end{gather*}
\subsection{Method of substitution}
The method of substitution is the integration analogue of the chain rule.

If
\[g\dd x=f(u)u^\prime\dd x,\]
that is, the differential $g\dd x$ is the result of a chain rule, then
\begin{alignat*}{1}
\int g\dd x & =\int f(u)u^\prime\dd x\\
            & =\int f(u)\dd u\\
			& =F(u)+C,
\end{alignat*}
where $F(u)$ is an antiderivative of $f(u)$.

\section{Differential equation}
\subsection{Separation of variables}
A differential equation is \textbf{separable} is it can be written in the form $\frac{\dd y}{\dd x}=f(x)g(y)$.

To solve a separable differential equation, separate all terms involving $x$ from terms involving $y$, and anti-differentiate both sides.
\begin{gather*}
\frac{\dd y}{\dd x}=f(x)g(y)\\
\frac{\dd y}{g(y)}=f(x)\dd x\\
\int\frac{\dd y}{g(y)}=\int f(x)\dd x\\
\end{gather*}
\subsection{Initial conditions}
\subsubsection{Theorem}
Given a differential equation $\frac{\dd y}{\dd x}=f(x)g(y)$ and an \textbf{initial condition} $y(a)=b$, if $f$, $g$, and $g^\prime$ are continuous near $(a,b)$, then there is a unique function $y$ whose derivative is given by $f(x)g(y)$ and that passes through the point $(a,b)$.
\subsubsection{Slope fields}
The \textbf{slope field} is a diagram that helps us to visualize the information in a first order differential equation. The slope field is obtained as follows. At each point $(x,y)$. The solution curves must be tangent to the slope field at all points.
\subsubsection{Euler's method}
Given the differential equation $\frac{\dd y}{\dd x}=x+y$:
\begin{enumerate}
\item Choose a step size $h$ (The smaller the step size, the more accurate the approximation.)
\item Let $(x_0,y_0)$ be the initial condition.
\item We can use the differential equation and step size to determine the value of the function at $x_1$:
  \begin{align*}
  & x_1=x_0+h\\
  & y_1=y_0+(x_0+y_0)h\quad\text{(linear approximation)}
  \end{align*}
\item Iterate this process:
  \begin{align*}
  & x_{k+1}=x_k+h\\
  & y_{k+1}=y_k+(x_k+y_k)h\quad\text{(linear approximation)}
  \end{align*}
\end{enumerate}
